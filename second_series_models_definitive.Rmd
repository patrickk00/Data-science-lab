---
title: "R Notebook"
output:
  word_document:
    toc: yes
  pdf_document:
    latex_engine: xelatex
    keep_tex: yes
    number_sections: yes
    toc: yes
  html_document:
    toc: yes
    df_print: paged
encoding: "UTF-8"
---


##Libraries

```{r}
library(dplyr)
library(ggplot2)
library(forecast)
library(dplyr)
library(xgboost)
library(readr)
library(stringr)
library(caret)
library(randomForest)
library(caret)
library(prophet)
library(rstanarm)
library(dplyr)
library(prophet)
library(ggplot2)
library(forecast)
library(zoo)
library(car)
library(forecast)
library(fastDummies)
library(forecast)
library(imputeTS)

```

##Load data

Here, we analyze from 4/5/2020 until 3/5/23. In other words, from when they reopened after the first covid wave until the last available date. 

```{r}
# Read the CSV file with explicit encoding
setwd("~/Desktop")
df<- read.csv("df_pulito.csv", header = TRUE)

#df <- read.csv("C:/Users/pc/Desktop/r004_data.csv", header = TRUE)#, fileEncoding = "ISO-8859-1")
df$date <- as.Date(df$date, format = "%Y-%m-%d")
start_date <- as.Date("2020-05-04")
end_date <- as.Date("2023-03-3")
filtered_post <- df %>%
  filter(date >= start_date & date <= end_date)
filtered_post$Giorno <- iconv(filtered_post$Giorno, from = "ISO-8859-1", to = "UTF-8")
```

```{r}
filtered_post

```
#EDA

```{r}
ggplot(filtered_post, aes(x = date, y = lordo.totale)) +
  geom_line() +
  ggtitle("Lordo Totale Over Time") +
  xlab("Date") +
  ylab("Lordo Totale")
time_series <- ts(filtered_post$lordo.totale, start=c(2020, 5), frequency=365)
plot(time_series, main="Time Series of Lordo Totale", xlab="Year", ylab="Lordo Totale")
na_rows <- filtered_post[rowSums(is.na(filtered_post)) > 0,]
print(na_rows)
unique_days <- unique(filtered_post$Giorno)
print(unique_days)

```

There appears to be a seasonal pattern in the data. The sales seem to have repetitive peaks and valleys, suggesting a consistent seasonal influence. This could be linked to weekly cycles or other factors.
There's a downward trend around mid-2020, which could be related to the COVID-19 pandemic's impact, especially since there's a "Lockdown" column in the dataset.
Post this drop, there seems to be a recovery in the latter part of 2020 and beyond, though with some variability.
There are several sharp drops to near zero throughout the series. These could be days when the restaurant was closed or had very low sales.

The results of the Augmented Dickey-Fuller (ADF) test are:

    Test Statistic: −1.7253−1.7253
    p-value: 0.41810.4181
    Critical Values:
        1%: −3.4368−3.4368
        5%: −2.8644−2.8644
        10%: −2.5683−2.5683

Given that our test statistic (-1.7253) is greater than the critical values and the p-value is greater than 0.05, we fail to reject the null hypothesis. This indicates that the time series is non-stationary.



##Missing Values
```{r}
filtered_post$lordo.totale <- na.seadec(filtered_post$lordo.totale)
# Check if there are any NA values remaining after imputation
remaining_na <- sum(is.na(filtered_post$lordo.totale))
print(paste("Number of NA values after imputation:", remaining_na))
```

As the name suggests, the method involves seasonally decomposing the time series data to handle missing values effectively. Seasonal decomposition often involves breaking down a time series into its underlying components such as trend, seasonality, and residuals. By understanding these components, you can better estimate missing values using appropriate models.

https://journal.r-project.org/archive/2017/RJ-2017-009/RJ-2017-009.pdf


```{r}
na <- sum(is.na(filtered_post$lordo.totale))
na

```

##0. Function to perform Rolling Origin 

```{r}

library(forecast)
library(ggplot2)

rolling_forecast_function <- function(data, model_type = "auto.arima", h = c(7), origins = 20, regressors = NULL, boxcox = FALSE, lambda = NULL) {
  
  if (is.data.frame(data)){
    obs <- nrow(data)
  } else {
    obs <- length(data)
  }
  print(paste("Number of observations: ", obs))
  

  result_collector <- data.frame(Horizon = numeric(0), RMSE = numeric(0), MAPE = numeric(0))




  for (j in h){
    rmse_array <- c()
    mape_array <- c()
    best_rmse <- Inf  
    best_iteration <- 0 
    print(paste("Horizon: ", j))
      for (i in 1:origins) {
      print(paste("iteration: ", i, "Observation: ", obs, " origins: ", origins, "j: ",j))
      train_size= obs+i-origins-j
      print(paste("Train size: ", train_size))
      if (!is.data.frame(data)){

        train_set <- data[1:train_size]
        test_set <- data[(train_size + 1):(obs + i - origins)]

      } else {
        train_set <- data[1:train_size,]
        test_set <- data[(train_size + 1):(obs + i - origins),]
      }
  
      if (!is.null(regressors)){
        train_regressor <- regressors[1:train_size,]
        test_regressor <- regressors[(train_size + 1):obs,]
      }
      
      if (model_type == "auto.arima") {
        if (is.null(regressors)) {
          set.seed(1) 
          model <- auto.arima(train_set, seasonal=TRUE)#, stepwise=FALSE, approximation=FALSE)
          } else {
            if (boxcox){
               lambda <- BoxCox.lambda(train_set, method="guerrero", lower=-1, upper=2)
              train_set <- BoxCox(train_set, lambda)
            }
            model <- auto.arima(train_set, seasonal = TRUE, stepwise = FALSE,lambda = lambda, approximation = FALSE, xreg = train_regressor)
        }
      } 
      else if (model_type == "prophet"){
        prophet_df <- data.frame(ds = train_set$date, y = train_set$lordo.totale)
        set.seed(13)
        model <- prophet(prophet_df)
        future <- make_future_dataframe(model, periods = nrow(test_set))
        forecasts <- predict(model, future, daily.seasonality=TRUE)# adding later daily.seasonality
  
      } 
      else if (model_type == "tbats") {
          model <- tbats(train_set, seasonal.periods=7)
      } 
      else if (model_type == "holt-winters") {
          model <- HoltWinters(ts(train_set, frequency=7), seasonal="additive")
      } 
      else if (model_type == "ets") {
          model <- ets(train_set, model = 'ANN')
      } 
      else if (model_type == "random.forest"){
        model <- randomForest(lordo.totale ~ Giorno + Weekend_New + Festivo_or_Weekend_New + lordo.totale_lag1 + Colore+lordo.totale_lag2 + lordo.totale_lag7, data=train_set)
      } 
      else if (model_type == "random.forest.lag") {
        model <- randomForest(lordo.totale ~ lag_1 + lag_7, data = train_set)

      }
      else {
        stop("Invalid model_type. Supported values are 'auto.arima', 'ets', 'tbats and 'holt-winters'")
      }
      
      if(model_type == 'auto.arima' || model_type == 'tbats' || model_type == 'ets' || model_type == 'holt-winters'){
        if(is.null(regressors) ){
       
          forecasts <- forecast(model, h=length(test_set))
          
        }else{
          print(paste("TEST REGRESSORS: ", test_regressor))

          print(paste("TEST SET: ", test_set))
          print(paste(model))
          print(paste("Number of regressors in the model: ", ncol(model$x)))
          print(paste("Number of regressors in the test data: ", ncol(test_regressor)))


          forecasts <- forecast(model, xreg=test_regressor, h=length(test_set))

           if(boxcox){
            forecasts$mean <- InvBoxCox(forecasts$mean, lambda)

          }
        }
      }
      else if (model_type == 'random.forest' || model_type == 'random.forest.lag') {
        forecasts <- predict(model, newdata=test_set)
  
      }
      #EVALUATION

      if (model_type == 'random.forest' || model_type == 'random.forest.lag'){
        rmse_test <- sqrt(mean((test_set$lordo.totale - forecasts)^2))
        mape_test <- mean(abs((test_set$lordo.totale - forecasts) / test_set$lordo.totale)) * 100
  
      }
      else if (model_type != 'prophet'){
        rmse_test <- sqrt(mean((test_set - forecasts$mean)^2))
        mape_test <- mean(abs((test_set - forecasts$mean) / test_set)) * 100
      } else {
        print("CHICHI")
        #print(paste("FORECAST:",forecasts))
        #print(paste("train: ",forecasts))
        rmse_test <- sqrt(mean((test_set$lordo.totale - forecasts$yhat[(nrow(train_set)+1):nrow(forecasts)])^2))
        mape_test <- mean(abs((test_set$lordo.totale - forecasts$yhat[(nrow(train_set) + 1):nrow(forecasts)]) / test_set$lordo.totale), na.rm = TRUE) * 100
        print(paste("RMSE on the test set:", rmse(test_set$lordo.totale, forecasts$yhat[(nrow(train)+1):nrow(forecasts)])))
        print(paste("MAPE on the test_set set:", MAPE(test_set$lordo.totale, forecasts$yhat[(nrow(train)+1):nrow(forecasts)])))

  
      }
      
      # Print the RMSE
      print(paste("RMSE on the test set:", rmse_test))
      print(paste("MAPE on the test set:", mape_test))
        
      rmse_array <- c(rmse_array, rmse_test)
      mape_array <- c(mape_array, mape_test)
  
      }
    
     
      result_collector[nrow(result_collector) + 1,] = c(j,mean(rmse_array), mean(mape_array))

      plot(forecasts)



  }



  return(list(result_collector, forecasts, model))
}


```






##1. ETS
https://otexts.com/fpp2/estimation-and-model-selection.html
```{r}
filtered_post$Giorno <- gsub("lunedÃ¬", "lunedi", filtered_post$Giorno)
filtered_post$Giorno <- gsub("martedÃ¬", "martedi", filtered_post$Giorno)
filtered_post$Giorno <- gsub("mercoledÃ¬", "mercoledi", filtered_post$Giorno)
filtered_post$Giorno <- gsub("giovedÃ¬", "giovedi", filtered_post$Giorno)
filtered_post$Giorno <- gsub("venerdÃ¬", "venerdi", filtered_post$Giorno)
filtered_post$Giorno <- gsub("sabato", "sabato", filtered_post$Giorno)
filtered_post$Giorno <- gsub("domenica", "domenica", filtered_post$Giorno)
filtered_tmp <- filtered_post %>%
  filter(ristorante == 'R003') 
vendite_ts <- ts(filtered_tmp$lordo.totale)
data <- vendite_ts
train_size <- floor(0.8 * length(vendite_ts))
train_vendite <- vendite_ts[1:train_size]
test_vendite <- vendite_ts[(train_size + 1):length(vendite_ts)]
ets_model <- ets(train_vendite)
forecast_ets <- forecast(ets_model, h=length(test_vendite))
rmse_ets <- sqrt(mean((test_vendite - forecast_ets$mean)^2))
print(paste("RMSE for ETS:", rmse_ets))
```






###1.1 Rolling Origin ETS
```{r}


filtered_tmp <- filtered_post %>%
  filter(ristorante == 'R005' & lordo.totale != 0) 
data <- ts(filtered_tmp$lordo.totale, frequency = 1)
length(data)
result <- rolling_forecast_function(data, model_type = "ets", h = c(1,7,14,30,60), origins = 20, boxcox = TRUE)
result <- result[[1]]
forecasts <- result[[2]]
model <- result[[3]]

print(paste("Results: ", result))

 
result
```



```{r}
my_empty_list <- list()

# Append data using append() function
my_empty_list <- append(my_empty_list, "Data1")
my_empty_list <- append(my_empty_list, "Data2")
my_empty_list[0]
```
```{r}
# Preparazione dei dati per Prophet
prophet_df <- data.frame(ds = filtered_post$date, y = filtered_post$lordo.totale)
dummies_giorno <- model.matrix(~ Giorno - 1, data = filtered_post)
dummies_colore <- model.matrix(~ Colore - 1, data = filtered_post)
prophet_df <- cbind(prophet_df, dummies_giorno, dummies_colore)

filtered_post$Lockdown <- as.logical(filtered_post$Lockdown)
filtered_post$Weekend_New <- as.logical(filtered_post$Weekend_New)

prophet_df$Lockdown_1 <- as.integer(filtered_post$Lockdown)

prophet_df$Lockdown_0 <- as.integer(!filtered_post$Lockdown)
prophet_df$Weekend_1 <- as.integer(filtered_post$Weekend_New)
prophet_df$Weekend_0 <- as.integer(!filtered_post$Weekend_New)
h_values <- c(1, 7, 14)  # Orizzonti di previsione più corti
# Imposta la lunghezza del training set
initial_train_length <- 1010#floor(0.999 * nrow(prophet_df))

# Inizializzazione del dataframe per i risultati RMSE
results <- data.frame(ds=character(), RMSE=double())

for (t in (initial_train_length + 1):(nrow(prophet_df))) {
  print(paste("Iterationes: ", t))
  train_df <- prophet_df[1:t, ]
  test_df <- prophet_df[(t + 1):(t + 1), ]
  
  # Creazione e addestramento del modello Prophet
  model_prophet <- prophet(daily.seasonality = TRUE)
  regressors <- setdiff(names(prophet_df), c("ds", "y"))
  for (col in regressors) {
    model_prophet <- add_regressor(model_prophet, name = col)
  }
  model_prophet <- fit.prophet(model_prophet, train_df)
  
  # Previsione sul test set
  future <- make_future_dataframe(model_prophet, periods = 1)
  # Assicurati che future_extended abbia lo stesso numero di righe di future
  future_extended <- cbind(future, prophet_df[(t + 1):(t + 1), regressors])

  predictions <- predict(model_prophet, future_extended)
  rmse <- sqrt(mean((test_df$y - predictions$yhat)^2, na.rm = TRUE))

  # Salvataggio dei risultati RMSE
  results <- rbind(results, data.frame(ds=test_df$ds[1], RMSE=rmse))
}

# Stampa il dataframe dei risultati RMSE
print(results)
```
```{r}
filtered_post
```


###1.2 Non seasonal ETS
```{r}
library(forecast)
# Non-seasonal Exponential Smoothing
es_model <- ets(train_vendite, lambda=TRUE,model="ANN") # "ANN" specifies a non-seasonal error, additive trend, no seasonality.
forecast_es <- forecast(es_model, h=length(test_vendite))
rmse_es <- sqrt(mean((test_vendite - forecast_es$mean)^2))
print(paste("RMSE for Non-seasonal Exponential Smoothing:", rmse_es))

```

##2. PROPHET

https://facebook.github.io/prophet/docs/seasonality,_holiday_effects,_and_regressors.html#modeling-holidays-and-special-events 
We can also add specific holiday dates to help them understand when it's a holiday or if there's something special, it has been done on purpose.

```{r}
df_prophet <- data.frame(ds = as.Date(filtered_post$date), y = filtered_post$lordo.totale)
train_prophet <- df_prophet[1:train_size,]
test_prophet <- df_prophet[(train_size + 1):nrow(df_prophet),]
prophet_model <- prophet(train_prophet,daily.seasonality=TRUE)
forecast_prophet <- predict(prophet_model, test_prophet)
rmse_prophet <- sqrt(mean((test_prophet$y - forecast_prophet$yhat)^2))
print(paste("RMSE for Prophet:", rmse_prophet))

```
```{r}
filtered_post$Lockdown <- as.logical(filtered_post$Lockdown)
filtered_post$Weekend_New <- as.logical(filtered_post$Weekend_New)
filtered_post$Weekend_New
```


###2.1 PROPHET+regressors

```{r}
prophet_df <- data.frame(ds = filtered_post$date, y = filtered_post$lordo.totale)
dummies_giorno <- model.matrix(~ Giorno - 1, data = filtered_post)
dummies_colore <- model.matrix(~ Colore - 1, data = filtered_post)
prophet_df <- cbind(prophet_df, dummies_giorno, dummies_colore)
prophet_df$Lockdown_1 <- as.integer(filtered_post$Lockdown)
prophet_df$Lockdown_0 <- as.integer(!filtered_post$Lockdown)
prophet_df$Weekend_1 <- as.integer(filtered_post$Weekend_New)
prophet_df$Weekend_0 <- as.integer(!filtered_post$Weekend_New)
train_size <- floor(0.8 * nrow(prophet_df))
train_df <- prophet_df[1:train_size, ]
test_df <- prophet_df[(train_size + 1):nrow(prophet_df), ]
model_prophet <- prophet(daily.seasonality = TRUE)
for (col in colnames(dummies_giorno)) {
    model_prophet <- add_regressor(model_prophet, name = col)
}
for (col in colnames(dummies_colore)) {
    model_prophet <- add_regressor(model_prophet, name = col)
}
model_prophet <- add_regressor(model_prophet, name = "Lockdown_1")
model_prophet <- add_regressor(model_prophet, name = "Lockdown_0")
model_prophet <- add_regressor(model_prophet, name = "Weekend_1")
model_prophet <- add_regressor(model_prophet, name = "Weekend_0")

# Fit the model on the training data
model_prophet <- fit.prophet(model_prophet, train_df)

# Predict on the test set
future <- data.frame(ds = test_df$ds,freq = 'm') #boh provo la moltiplicativa perchè nell'hot winters era andata bene
future <- cbind(future, dummies_giorno[(train_size + 1):nrow(prophet_df), ])
future <- cbind(future, dummies_colore[(train_size + 1):nrow(prophet_df), ])
future$Lockdown_1 <- test_df$Lockdown_1
future$Lockdown_0 <- test_df$Lockdown_0
future$Weekend_1 <- test_df$Weekend_1
future$Weekend_0 <- test_df$Weekend_0
predictions <- predict(model_prophet, future)
rmse <- sqrt(mean((test_df$y - predictions$yhat)^2))
print(paste("RMSE for Prophet:", rmse))
print(model_prophet$params$beta)

```


###2.2 Rolling Origin PROPHET

```{r}
result <- rolling_forecast_function(filtered_post, model_type = "prophet", h = c(7,14,23,70), origins = 20, )
result <- result[[1]]
forecasts <- result[[2]]
model <- result[[3]]

print(paste("Results: ", result))

 
result
```





###2.3 PROPHET with days 


```{r}

#MARKED DOWN
filtered_post$Giorno <- gsub("lunedÃ¬", "lunedi", filtered_post$Giorno)
filtered_post$Giorno <- gsub("martedÃ¬", "martedi", filtered_post$Giorno)
filtered_post$Giorno <- gsub("mercoledÃ¬", "mercoledi", filtered_post$Giorno)
filtered_post$Giorno <- gsub("giovedÃ¬", "giovedi", filtered_post$Giorno)
filtered_post$Giorno <- gsub("venerdÃ¬", "venerdi", filtered_post$Giorno)
filtered_post$Giorno <- gsub("sabato", "sabato", filtered_post$Giorno)
filtered_post$Giorno <- gsub("domenica", "domenica", filtered_post$Giorno)

#Create the dataframe for Prophet with 'ds' for dates and 'y' for values
prophet_df <- data.frame(ds = filtered_post$date, y = filtered_post$lordo.totale)

#Create dummy variables for 'Giorno'
dummies <- model.matrix(~ Giorno - 1, data = filtered_post)
prophet_df <- cbind(prophet_df, dummies)

train_size <- floor(0.8 * nrow(prophet_df))
prophet_train <- prophet_df[1:train_size, ]
prophet_test <- prophet_df[(train_size + 1):nrow(prophet_df), ]
model_prophet <- prophet(yearly.seasonality = TRUE)

#Add dummy variables for 'Giorno' to the model
for (col in colnames(dummies)) {
  model_prophet <- add_regressor(model_prophet, name = col)
}

#Fit the Prophet model
model_prophet <- fit.prophet(model_prophet, prophet_train)

#Prepare future dataframe
future <- make_future_dataframe(model_prophet, periods = nrow(prophet_test))

#Add dummy variables for 'Giorno' to the future dataframe
future_dummies <- model.matrix(~ Giorno - 1, data = filtered_post[(train_size + 1):nrow(filtered_post), ])
future <- cbind(future, future_dummies)
Forecast
forecasts_prophet <- predict(model_prophet, future)
rmse_prophet <- sqrt(mean((prophet_test$y - forecasts_prophet$yhat[(train_size + 1):nrow(prophet_df)])^2))
print(paste("RMSE for Prophet:", rmse_prophet))
plot(model_prophet, forecasts_prophet)

```


```{r}
summary(model_prophet)

```

##3. Holt-Winters 

```{r}
vendite_ts <- ts(filtered_post$lordo.totale,  frequency = (365.25/7))
train_size <- floor(0.8 * length(vendite_ts))
train_vendite <- vendite_ts[1:train_size]
test_vendite <- vendite_ts[(train_size + 1):length(vendite_ts)]
hw_model <- HoltWinters(train_vendite,seasonal = "multiplicative" ,beta = FALSE, gamma = FALSE)
forecast_hw <- forecast(hw_model, h=length(test_vendite))
rmse_hw <- sqrt(mean((test_vendite - forecast_hw$mean)^2))
print(paste("RMSE for Holt-Winters with Additive Seasonality:", rmse_hw))

```

###3.1 Rolling Forecast Origin Holt-Winters 

```{r}
result <- rolling_forecast_function(data, model_type = "holt-winters", h = c(7,14,23,70), origins = 20, regressors = NULL)

result <- result[[1]]
forecasts <- result[[2]]
model <- result[[3]]

print(paste("Results: ", result))

 
result
```


```{r}
unique(filtered_post$Giorno)
unique(filtered_post$Weekend)
unique(filtered_post$Festivo_or_Weekend)
unique(filtered_post$Festivo)
unique(filtered_post$Colore)
unique(filtered_post$Lockdown)

```
##4. Random Forest

```{r}
head(filtered_post)
```


```{r}
library(randomForest)
filtered_post <- filtered_post %>%
  mutate(
    lordo.totale_lag1 = lag(lordo.totale, 1),
    lordo.totale_lag2 = lag(lordo.totale, 2),
    lordo.totale_lag7 = lag(lordo.totale, 7)
  )
#CREATE LAGS USED IN OTHER MODELS LATER
filtered_post$Giorno <- as.factor(filtered_post$Giorno)
filtered_post$Colore <- as.factor(filtered_post$Colore)
filtered_post <- na.omit(filtered_post)
train_size <- floor(0.8 * nrow(filtered_post))
train_df <- filtered_post[1:train_size, ]
test_df <- filtered_post[(train_size + 1):nrow(filtered_post), ]
set.seed(3)
rf_model <- randomForest(lordo.totale ~ Giorno + Weekend_New + Festivo_or_Weekend_New + lordo.totale_lag1 + Colore+lordo.totale_lag2 + lordo.totale_lag7, data=train_df)
print(rf_model)
predictions <- predict(rf_model, newdata=test_df)
importance(rf_model)
varImpPlot(rf_model)
rmse <- sqrt(mean((test_df$lordo.totale - predictions)^2))
print(rmse)
```

###4.1 Rolling Forecast Origin Random Forest

```{r}
result <- rolling_forecast_function(filtered_post, model_type = "random.forest", h = c(1,7,14,30,60), origins = 100, regressors = NULL)
result <- result[[1]]
forecasts <- result[[2]]
model <- result[[3]]

print(paste("Results: ", result))

 
result
```



##5. Bayesian best features

```{r}
library(forecast)
filtered_post <- filtered_post %>%
  mutate(
    lordo.totale_lag1 = lag(lordo.totale, 1),
    lordo.totale_lag2 = lag(lordo.totale, 2),
    lordo.totale_lag7 = lag(lordo.totale, 7)
  )
train_size <- floor(0.8 * nrow(filtered_post))
train <- filtered_post[1:train_size, ]
test <- filtered_post[(train_size + 1):nrow(filtered_post), ]
set.seed(2304)
# Fit a Bayesian linear regression model with Giorno, multiple lags and possibly weekend
model_bayesian_with_lags <- stan_glm(lordo.totale ~ Giorno +Colore+ lordo.totale_lag1 + Lockdown+ rain + lordo.totale_lag7, 
                                     data = train)
summary(model_bayesian_with_lags)
predictions <- predict(model_bayesian_with_lags, newdata = test)
RMSE <- sqrt(mean((predictions - test$lordo.totale)^2))
print(RMSE)
residuals <- test$lordo.totale - predictions
ljung_box_test <- Box.test(residuals, type = "Ljung-Box")
print(ljung_box_test)
```





### 5.1 Bayesian with Lag 1 + days

library(forecast)
filtered_post <- filtered_post %>%
  mutate(
    lordo.totale_lag1 = lag(lordo.totale, 1),
    lordo.totale_lag2 = lag(lordo.totale, 2),
    lordo.totale_lag7 = lag(lordo.totale, 7)
  )

Convert character columns to factor

filtered_post$Giorno <- as.factor(filtered_post$Giorno)
filtered_post$Colore <- as.factor(filtered_post$Colore)


```{r}
library(forecast)
filtered_post <- filtered_post %>%
  mutate(
    lordo.totale_lag1 = lag(lordo.totale, 1),
    lordo.totale_lag2 = lag(lordo.totale, 2),
    lordo.totale_lag7 = lag(lordo.totale, 7)
  )
set.seed(1)
filtered_post$Giorno <- as.factor(filtered_post$Giorno)
filtered_post$Colore <- as.factor(filtered_post$Colore)
model_bayesian_with_lags <- stan_glm(lordo.totale ~ Giorno+ Colore+lordo.totale_lag1, 
                                     data = train)
summary(model_bayesian_with_lags)
predictions <- predict(model_bayesian_with_lags, newdata = test)
RMSE <- sqrt(mean((predictions - test$lordo.totale)^2))
print(RMSE)
residuals <- test$lordo.totale - predictions
ljung_box_test <- Box.test(residuals, type = "Ljung-Box")
print(ljung_box_test)

```

CREATE LAGS


```{r}
filtered_post <- filtered_post %>%
  mutate(
    lordo.totale_lag1 = lag(lordo.totale, 1),
    lordo.totale_lag2 = lag(lordo.totale, 2),
    lordo.totale_lag7 = lag(lordo.totale, 7)
  )
```


###5.2 Bayesian with most important features (red zone + rain)

```{r}
head(filtered_post)
```

```{r}
train_size <- floor(0.8 * nrow(filtered_post))
train <- filtered_post[1:train_size, ]
test <- filtered_post[(train_size + 1):nrow(filtered_post), ]
set.seed(2304)
# Fit a Bayesian linear regression model with Giorno, multiple lags and possibly weekend
model_bayesian_with_lags <- stan_glm(lordo.totale ~ Giorno +Colore+ lordo.totale_lag1 + Lockdown+ rain + lordo.totale_lag7, 
                                     data = train)
summary(model_bayesian_with_lags)
predictions <- predict(model_bayesian_with_lags, newdata = test)
RMSE <- sqrt(mean((predictions - test$lordo.totale)^2))
print(RMSE)
residuals <- test$lordo.totale - predictions
ljung_box_test <- Box.test(residuals, type = "Ljung-Box")
print(ljung_box_test)
```

###5.3 DA FARE Rolling forecast Origin Bayesian models

##7. ARIMAx with FOURIER 

```{r}
f <- filtered_post %>%
  filter(ristorante == 'R004') 
vendite_ts <- ts(f$lordo.totale , frequency = 365.25/7)
# Generate Fourier terms to capture seasonality. 
# K is the number of sine and cosine terms (the more terms, the more complex seasonality you can capture).
K <- 4
fourier_terms <- fourier(vendite_ts, K=K)
train_size <- floor(0.8 * length(vendite_ts))
train_vendite <- vendite_ts[1:train_size]
test_vendite <- vendite_ts[(train_size + 1):length(vendite_ts)]
train_fourier <- fourier_terms[1:train_size,]
test_fourier <- fourier_terms[(train_size + 1):nrow(fourier_terms),]
fit <- auto.arima(train_vendite, xreg=train_fourier, seasonal=TRUE)
forecasted <- forecast(fit, xreg=test_fourier, h=length(test_vendite))
checkresiduals(fit)
rmse <- sqrt(mean((test_vendite - forecasted$mean)^2))
print(paste("RMSE for ARIMA with Fourier terms:", rmse))
summary(fit)
```
###7.1ARIMAX FOURIER 2 PERIODS

```{r}
#vendite_ts <- ts(filtered_post$lordo.totale, frequency = 365.25/7)
fourier_weekly <- fourier(ts(vendite_ts, frequency=7), K=3)
fourier_yearly <- fourier(ts(vendite_ts, frequency=365.25), K=15)
fourier_combined <- cbind(fourier_weekly, fourier_yearly)
train_size <- floor(0.8 * length(vendite_ts))
train_vendite <- vendite_ts[1:train_size]
test_vendite <- vendite_ts[(train_size + 1):length(vendite_ts)]
train_fourier <- fourier_combined[1:train_size,]
test_fourier <- fourier_combined[(train_size + 1):nrow(fourier_combined),]
fit <- auto.arima(train_vendite, xreg=train_fourier, seasonal=FALSE)
checkresiduals(fit)
forecasted <- forecast(fit, xreg=test_fourier, h=length(test_vendite))
rmse <- sqrt(mean((test_vendite - forecasted$mean)^2))
print(paste("RMSE for ARIMA with Fourier terms:", rmse))

```

Not good results

### 7.2 FOURIER With colors
```{r}

#vendite_ts <- ts(filtered_post$lordo.totale, frequency = 365.25/7)
K <- 4
fourier_terms <- fourier(vendite_ts, K=K)
colore_dummies <- model.matrix(~ Colore - 1, data = f)[,-1]  # drop the first column
combined_xreg <- cbind(fourier_terms, colore_dummies)
train_size <- floor(0.8 * length(vendite_ts))
train_vendite <- vendite_ts[1:train_size]
test_vendite <- vendite_ts[(train_size + 1):length(vendite_ts)]
train_regressors <- combined_xreg[1:train_size,]
test_regressors <- combined_xreg[(train_size + 1):nrow(combined_xreg),]
fit <- auto.arima(train_vendite, xreg=train_regressors, seasonal=TRUE)
forecasted <- forecast(fit, xreg=test_regressors, h=length(test_vendite))
checkresiduals(fit)
rmse <- sqrt(mean((test_vendite - forecasted$mean)^2))
print(paste("RMSE for ARIMA with Fourier terms and 'Colore' dummies:", rmse))
summary(fit)


length(fourier_terms)
```
### 7.3 FOURIR CON GIORNO e colori

```{r}
#vendite_ts <- ts(filtered_post$lordo.totale, frequency = 365.25/7)
K <- 4
fourier_terms <- fourier(vendite_ts, K=K)
colore_dummies <- model.matrix(~ Colore - 1, data = f)[,-1]  # drop the first column
giorno_dummies <- model.matrix(~ Giorno - 1, data = f)[,-1]  # drop the first column
combined_xreg <- cbind(fourier_terms, colore_dummies, giorno_dummies)
train_size <- floor(0.8 * length(vendite_ts))
train_vendite <- vendite_ts[1:train_size]
test_vendite <- vendite_ts[(train_size + 1):length(vendite_ts)]
train_regressors <- combined_xreg[1:train_size,]
test_regressors <- combined_xreg[(train_size + 1):nrow(combined_xreg),]
fit <- auto.arima(train_vendite, xreg=train_regressors, seasonal=TRUE)
forecasted <- forecast(fit, xreg=test_regressors, h=length(test_vendite))
checkresiduals(fit)
rmse <- sqrt(mean((test_vendite - forecasted$mean)^2))
print(paste("RMSE for ARIMA with Fourier terms, 'Colore', and 'Giorno' dummies:", rmse))
summary(fit)

```


#8. ARIMAX 

### 8.1 ARIMAX GIORNO E COLORE FAILED

RESIDUALS OK BUT BAD PERFORMANCE
````markdown
<!-- ```{r}
library(dplyr)
library(lubridate)

weekly_data <- filtered_post %>%
  group_by(week = floor_date(date, "week")) %>%
  summarise(weekly_total = sum(lordo.totale))
vendite_weekly <- ts(weekly_data$weekly_total, frequency = 52)
weekly_predictors <- filtered_post %>%
  group_by(week = floor_date(date, "week")) %>%
  summarise(Giorno = first(Giorno), 
            Colore = first(Colore), 
            Lockdown = first(Lockdown)) %>%
  mutate(Giorno = as.factor(Giorno), 
         Colore = as.factor(Colore),
         Lockdown = as.numeric(Lockdown))


giorno_dummies <- model.matrix(~ Giorno - 1, data = weekly_predictors)[,-1]  # drop the first column
colore_dummies <- model.matrix(~ Colore - 1, data = weekly_predictors)[,-1]  # drop the first column





predictors <- cbind(giorno_dummies, colore_dummies, weekly_predictors$Lockdown)
train_size_weekly <- floor(0.8 * nrow(weekly_data))
train_vendite <- vendite_weekly[1:train_size_weekly]
test_vendite <- vendite_weekly[(train_size_weekly + 1):length(vendite_weekly)]
train_predictors <- predictors[1:train_size_weekly, ]
test_predictors <- predictors[(train_size_weekly + 1):nrow(predictors), ]
fit <- auto.arima(train_vendite, xreg = train_predictors, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
forecasted <- forecast(fit, xreg = test_predictors, h = length(test_vendite))
checkresiduals(fit)
rmse <- sqrt(mean((test_vendite - forecasted$mean)^2))
print(paste("RMSE for SARIMAX with 'Giorno', 'Colore:", rmse))
summary(fit)



##STESSA COSA MA CON I DATI GIORNALIERI





vendite_ts <- ts(filtered_post$lordo.totale, frequency = 365.25/7)
giorno_dummies <- model.matrix(~ Giorno - 1, data = filtered_post)[,-1]  # drop the first column
colore_dummies <- model.matrix(~ Colore - 1, data = filtered_post)[,-1]  # drop the first column
# Combine dummies and Lockdown variable
predictors <- cbind(giorno_dummies, colore_dummies, filtered_post$Lockdown)
# Split the data into train and test sets
train_size <- floor(0.8 * length(vendite_ts))
train_vendite <- vendite_ts[1:train_size]
test_vendite <- vendite_ts[(train_size + 1):length(vendite_ts)]
train_predictors <- predictors[1:train_size, ]
test_predictors <- predictors[(train_size + 1):nrow(predictors), ]
fit <- auto.arima(train_vendite, xreg = train_predictors, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
forecasted <- forecast(fit, xreg = test_predictors, h = length(test_vendite))
checkresiduals(fit)
rmse <- sqrt(mean((test_vendite - forecasted$mean)^2))
print(paste("RMSE for SARIMAX with 'Giorno', 'Colore',:", rmse))
summary(fit)



```


###8.2 ARIMAX CON LAGGED,FAILED

NOT ROBUST
domanda su quest cosa: https://stats.stackexchange.com/questions/400897/xgboost-for-time-series-using-lag-of-target-variables

Lagged Variables (Autoregressive terms): The inclusion of lagged variables of the target in the model essentially makes it an autoregressive model. In the ARIMA framework, these are the AR terms. When you include them as external regressors, you're essentially reinforcing this autoregressive nature. The coefficients of these lagged variables show how much previous values of the series influence the current value.

````markdown
<!-- ```{r}
library(forecast)
# Define the time series
vendite_ts <- ts(filtered_post$lordo.totale, frequency = 365.25/7)
filtered_post$lag_1 <- lag(filtered_post$lordo.totale, 1)
filtered_post$lag_7 <- lag(filtered_post$lordo.totale, 7)
colore_dummies <- model.matrix(~ Colore - 1, data = filtered_post)[,-1]  # drop the first column
giorno_dummies <- model.matrix(~ Giorno - 1, data = filtered_post)[,-1]  # drop the first column
combined_xreg <- cbind(colore_dummies, giorno_dummies, filtered_post$lag_1, filtered_post$lag_7)
filtered_post <- filtered_post[8:nrow(filtered_post), ]
combined_xreg <- combined_xreg[8:nrow(combined_xreg), ]
vendite_ts <- vendite_ts[8:length(vendite_ts)]
train_size <- floor(0.8 * length(vendite_ts))
train_vendite <- vendite_ts[1:train_size]
test_vendite <- vendite_ts[(train_size + 1):length(vendite_ts)]
train_predictors <- combined_xreg[1:train_size,]
test_predictors <- combined_xreg[(train_size + 1):nrow(combined_xreg),]
fit <- auto.arima(train_vendite, xreg=train_predictors, seasonal=TRUE)
forecasted <- forecast(fit, xreg=test_predictors, h=length(test_vendite))
checkresiduals(fit)
rmse <- sqrt(mean((test_vendite - forecasted$mean)^2))
print(paste("RMSE for SARIMAX with 'Colore', 'Giorno', and Lagged Variables:", rmse))
summary(fit)


```

###8.3 ARIMAX 1,2,7 LAGGED  (Excellent)
```{r}
# Create lagged variables
filtered_post <- filtered_post %>%
  filter(ristorante == 'R003') 
filtered_post$lag_1 <- lag(filtered_post$lordo.totale, 1)
filtered_post$lag_2 <- lag(filtered_post$lordo.totale, 2)
filtered_post$lag_7 <- lag(filtered_post$lordo.totale, 7)
filtered_post <- filtered_post[8:nrow(filtered_post), ]
colore_dummies <- model.matrix(~ Colore, data = filtered_post)[,-1]  # Drop first column
giorno_dummies <- model.matrix(~ Giorno, data = filtered_post)[,-1] 
combined_xreg <- cbind(colore_dummies, giorno_dummies, filtered_post$lag_1, filtered_post$lag_2, filtered_post$lag_7)
vendite_ts <- ts(filtered_post$lordo.totale, frequency = 1)
train_size <- floor(0.8 * length(vendite_ts))
train_vendite <- vendite_ts[1:train_size]
test_vendite <- vendite_ts[(train_size + 1):length(vendite_ts)]
train_predictors <- combined_xreg[1:train_size, ]
test_predictors <- combined_xreg[(train_size + 1):nrow(combined_xreg), ]
fit <- auto.arima(train_vendite, xreg=train_predictors, seasonal=TRUE)
forecasted <- forecast(fit, xreg=test_predictors, h=length(test_vendite))
checkresiduals(fit)
rmse <- sqrt(mean((test_vendite - forecasted$mean)^2))
print(paste("RMSE for SARIMAX with 'Colore', 'Giorno', and Lagged Variables:", rmse))
summary(fit)


```
```{r}
filtered_post <- filtered_post %>%
    filter(lordo.totale != 0)
d <- filtered_post
d$lag_1 <- lag(d$lordo.totale, 1)
d$lag_2 <- lag(d$lordo.totale, 2)
d$lag_7 <- lag(d$lordo.totale, 7)
d <- d[8:nrow(d), ]
colore_dummies <- model.matrix(~ Colore, data = d)[,-1]  # Drop first column
giorno_dummies <- model.matrix(~ Giorno, data = d)[,-1] 
combined_xreg <- cbind(colore_dummies, giorno_dummies, d$lag_1, d$lag_2, d$lag_7)
vendite_ts <- ts(d$lordo.totale, frequency = 1)
train_size <- floor(0.8 * length(vendite_ts))
train_vendite <- vendite_ts[1:train_size]
test_vendite <- vendite_ts[(train_size + 1):length(vendite_ts)]
train_predictors <- combined_xreg[1:train_size, ]
test_predictors <- combined_xreg[(train_size + 1):nrow(combined_xreg), ]
fit <- auto.arima(train_vendite, xreg=train_predictors, seasonal=TRUE)
forecasted <- forecast(fit, xreg=test_predictors, h=length(test_vendite))
rmse <- sqrt(mean((test_vendite - forecasted$mean)^2))
print(paste("RMSE for SARIMAX with 'Colore', 'Giorno', and Lagged Variables:", rmse))
mape_test_ets <- mean(abs((test_vendite - forecasted$mean) / test_vendite)) * 100

print(paste("MAPE on the test set:", mape_test_ets))
```




    Ljung-Box Test: The p-value of 0.7812 indicates that the residuals are white noise. This means the residuals don't exhibit significant autocorrelation, which is a good sign. It suggests that the model has captured the underlying temporal structure well.

    RMSE: The RMSE (Root Mean Squared Error) is 6691.73. This metric provides an absolute measure of fit, indicating the average magnitude of the prediction error. This gives a sense of the average deviation from the true values. Whether this value is good or not often depends on the scale of your target variable and domain considerations.

    ARIMA Coefficients:
        AR (Autoregressive) terms: Your ARIMA model has 5 AR terms. All coefficients are significant given the standard errors. They represent the relationship of the current value of the series to its past values.
        MA (Moving Average) terms: You have 3 MA terms. These account for the relationship between an observation and a residual error from a moving average model applied to lagged observations.
        The drift term represents a constant linear or deterministic trend. Your drift is 11.5471, suggesting a positive linear trend in the series.

    Regressor Coefficients:
        ColoreBianca: The coefficient is 4716.32 with a standard error of 818.18. This means that, on average, days with the "Bianca" color label have sales higher by approximately 4716 units compared to the reference level.
        ColoreGialla: Similarly, days with the "Gialla" color label have sales higher by approximately 3313.89 units.
        ColoreRossa: Days with the "Rossa" color label have sales lower by approximately 3454.81 units.
        Giorno dummies: The coefficients represent the average difference in sales for each day compared to the reference level (in this case, probably "domenica"). For instance, sales on "lunedì" are higher by about 2526.97 units on average.

    Model Fit:
        The log-likelihood is -7715.48, which gives a measure of how well the model fits the data (higher is better).
        The AIC, AICc, and BIC values are measures of the relative quality of statistical models. Lower values are preferable.

    Other Metrics:
        MASE (Mean Absolute Scaled Error) is 0.467. This metric is relative to a naive forecast, and values less than one indicate that the model forecasts better than a naive baseline.
        The ACF1 value of 0.0026 indicates a very low autocorrelation in the residuals at lag 1.



###8.4 Rolling forecasting Origin ARIMAX Lagged
```{r}

# dum_day_df <- filtered_post[, c("date", "Giorno")]
# dum_day_df$Giorno <- as.factor(dum_day_df$Giorno)
# dum_day_df <- dummy_cols(dum_day_df, select_columns = c("Giorno"), remove_selected_columns = TRUE)
# 
# dum_day_df <- dum_day_df[ , !(names(dum_day_df) %in% c("date", "Giorno_lunedì"))]
# 
# data <- ts(filtered_post$lordo.totale, frequency = 1)

filtered_post <- filtered_post %>%
    filter(lordo.totale != 0)
d <- filtered_post
d$lag_1 <- lag(d$lordo.totale, 1)
d$lag_2 <- lag(d$lordo.totale, 2)
d$lag_7 <- lag(d$lordo.totale, 7)
d <- d[8:nrow(d), ]
colore_dummies <- model.matrix(~ Colore, data = d)[,-1]  # Drop first column
giorno_dummies <- model.matrix(~ Giorno, data = d)[,-1] 
combined_xreg <- cbind(colore_dummies, giorno_dummies, d$lag_1, d$lag_2, d$lag_7)


result <- rolling_forecast_function(d$lordo.totale, model_type = "auto.arima", h = c(7,14,30,60, 196), origins = 5, regressors = combined_xreg, boxcox = FALSE)
result <- result[[1]]
forecasts <- result[[2]]
model <- result[[3]]

print(paste("Results: ", result))

 
result
```


```{r}

data <- ts(filtered_post$lordo.totale, frequency = 1)

result <- rolling_forecast_function(data, model_type = "auto.arima", h = c(70), origins = 1, regressors = combined_xreg, boxcox = FALSE)
mean_rmse <- result[[1]]
mean_mape <- result[[2]]
forecasts <- result[[3]]
model <- result[[4]]

print(paste("Mean RMSE: ", mean_rmse))
print(paste("Mean MAPE: ", mean_mape))

 
plot(forecasts)
```



#SIMPLE SARIMA (FAILED)

````markdown
<!-- ```{r}
 Assuming data, start_date, and end_date are predefined

 Filtering the data
filtered_data <- data %>% filter(date >= start_date & date <= end_date)

Fitting the ARIMA model without the log transformation
model <- auto.arima(filtered_data$lordo.totale, seasonal = TRUE)

Forecasting 
predictions <- forecast(model, h=length(filtered_data$lordo.totale))$mean

Calculating RMSE
rmse_val <- rmse(filtered_data$lordo.totale, predictions)

Getting residuals and performing checks
residuals <- residuals(model)
checkresiduals(model)
plot(residuals, main="Residuals from ARIMA model")

ljung_box_test <- Box.test(residuals, type = "Ljung-Box", lag=7)

 Printing results
checkresiduals(model)
print(rmse_val)
print(ljung_box_test)

```

###PROVO IL LOG FAILED






````markdown
<!-- ```{r}

library(tidyverse)
library(lubridate)
library(forecast)
library(tseries)
library(Metrics)

filtered_data <- data %>% filter(date >= start_date & date <= end_date)
filtered_data$lordo.totale_log <- log(filtered_data$lordo.totale + 1)



model <- auto.arima(filtered_data$lordo.totale_log, seasonal = TRUE)
predictions_log <- forecast(model, h=length(filtered_data$lordo.totale_log))
predictions <- exp(predictions_log$mean) - 1
rmse_val <- rmse(filtered_data$lordo.totale, predictions)
residuals <- residuals(model)
checkresiduals(model)
plot(residuals, main="Residuals from SARIMA model on log-transformed data")
ljung_box_test <- Box.test(residuals, type = "Ljung-Box", lag=10)

checkresiduals(model)
print(rmse_val)
print(ljung_box_test)

```
###FAILED

````markdown
<!-- ```{r}
library(forecast)
library(ggplot2)

Define a function to fit ARIMA, visualize residuals, and perform the Ljung-Box test
analyze_arima <- function(data, p, d, q) {
  model <- arima(data, order=c(p, d, q))
  
  Plot residuals
  residuals <- ts(resid(model))
  ggplot(data = as.data.frame(residuals), aes(x = residuals)) + 
    geom_histogram(aes(y = ..density..), fill = "skyblue", color = "black") + 
    geom_density(aes(y = ..density..), color = "red") +
    ggtitle(paste("Histogram of Residuals for ARIMA(", p, ",", d, ",", q, ")", sep=""))
  
  ACF and PACF plots
  par(mfrow=c(2,1))
  acf(residuals, main=paste("ACF for ARIMA(", p, ",", d, ",", q, ")", sep=""))
  pacf(residuals, main=paste("PACF for ARIMA(", p, ",", d, ",", q, ")", sep=""))
  
   Ljung-Box test
  lb_test <- Box.test(residuals, type = "Ljung-Box", lag=10)
  print(lb_test)
  
  return(model)
}

 Fit and analyze ARIMA(6,1,4)
model_614 <- analyze_arima(filtered_data$lordo.totale, 6, 1, 4)
checkresiduals(model_614)
Fit and analyze ARIMA(7,1,4)
model_714 <- analyze_arima(filtered_data$lordo.totale, 7, 1, 4)
checkresiduals(model_714)
Fit and analyze ARIMA(8,1,4)
model_814 <- analyze_arima(filtered_data$lordo.totale, 7, 1, 7)
checkresiduals(model_814)

```


````markdown
<!-- ```{r}
library(forecast)

# Split the data
n <- length(filtered_data$lordo.totale)
train <- filtered_data$lordo.totale[1:(0.8*n)]
test <- filtered_data$lordo.totale[(0.8*n + 1):n]
model_717 <- arima(train, order = c(7,1,4))
summary(model_717)
forecasts <- forecast(model_717, h = length(test))
checkresiduals(model_717)
rmse_test <- sqrt(mean((test - forecasts$mean)^2))
print(paste("RMSE on the test set:", rmse_test))
plot(residuals, main="Residuals from SARIMA model on log-transformed data")
ljung_box_test <- Box.test(residuals, type = "Ljung-Box", lag=10)
print(ljung_box_test)
```

````markdown
<!-- ```{r}
library(forecast)

Define training and test data
n <- length(filtered_data$lordo.totale)
train <- filtered_data$lordo.totale[1:(0.8*n)]
test <- filtered_data$lordo.totale[(0.8*n + 1):n]

 Fit a SARIMA(7,1,4)(0,1,0)[7] model
 The seasonal order (0,1,0)[7] implies a weekly difference.
model_sarima <- arima(train, order=c(7,1,6), seasonal=list(order=c(0,1,0), period=7))
model_sarima <- auto.arima(train)
 Summary of the model
summary(model_sarima)

 Forecast the test data
forecasts <- forecast(model_sarima, h = length(test))

 Check residuals
checkresiduals(model_sarima)
Calculate RMSE for the test set
rmse_test <- sqrt(mean((test - forecasts$mean)^2))
print(paste("RMSE on the test set:", rmse_test))

Plot residuals
residuals <- residuals(model_sarima)
plot(residuals, main="Residuals from SARIMA model with weekly differencing")

Perform Ljung-Box test
ljung_box_test <- Box.test(residuals, type = "Ljung-Box", lag=10)
print(ljung_box_test)

```

````markdown
<!-- ```{r}

library(forecast)

Log-transform the data
log_data <- log(filtered_data$lordo.totale)

Apply Box-Cox transformation
lambda <- BoxCox.lambda(log_data)
box_cox_data <- BoxCox(log_data, lambda)

Differencing to remove trend and seasonality
diff_box_cox_data <- diff(box_cox_data, lag = 7)  # lag of 7 for weekly seasonality

Split into Train and Test Sets
n <- length(diff_box_cox_data)
train <- diff_box_cox_data[1:(0.8*n)]
test <- diff_box_cox_data[(0.8*n + 1):n]

Fit ARIMA model using auto.arima
model_arima <- auto.arima(train, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)

 Print model summary
summary(model_arima)

Forecast the test data
forecasts <- forecast(model_arima, h = length(test))

rmse_test <- sqrt(mean((test - forecasts$mean)^2))
print(paste("RMSE on the test set:", rmse_test))

residuals <- residuals(model_arima)
plot(residuals, main="Residuals from ARIMA model with weekly differencing and Box-Cox transformed data")


ljung_box_test <- Box.test(residuals, type = "Ljung-Box", lag=10)
print(ljung_box_test)




```

filtered_post <- filtered_post %>%
  mutate(week = lubridate::isoweek(date)) %>%
  group_by(week) %>%
  summarise(lordo.totale = mean(lordo.totale, na.rm = TRUE))




filtered_data <- filtered_data %>%
  mutate(week = lubridate::isoweek(date)) %>%
  group_by(week) %>%
  summarise(lordo.totale = mean(lordo.totale, na.rm = TRUE))
```{r}
plot(train)
```


#9 ARIMA Weekly data (OK)

```{r}
# Filter, add the dummy variable for 'Colore', and aggregate by ISO week
filtered_post <- df %>%
  filter(date >= start_date & date <= end_date & ristorante == "R004") %>%
  mutate(rosso_dummy = ifelse(Colore == "Rossa", 1, 0)) %>%
  mutate(week = lubridate::isoweek(date)) %>%
  group_by(week) %>%
  summarise(lordo.totale = mean(lordo.totale, na.rm = TRUE))


n <- length(filtered_post$lordo.totale)
train <- filtered_post$lordo.totale[1:(0.8*n)]
test <- filtered_post$lordo.totale[(0.8*n + 1):n]

# Fit a SARIMA(7,1,4)(0,1,0)[7] model
model_sarima <- auto.arima(train, seasonal = TRUE)

# Forecast the test data
forecasts <- forecast(model_sarima, h = length(test))
summary(model_sarima)
# Calculate RMSE for the test set
rmse_test <- sqrt(mean((test - forecasts$mean)^2))
print(paste("RMSE on the test set:", rmse_test))

# Plot residuals
residuals <- residuals(model_sarima)
plot(residuals, main="Residuals from SARIMA model with weekly aggregation")
checkresiduals(model_sarima)
# Perform Ljung-Box test
ljung_box_test <- Box.test(residuals, type = "Ljung-Box", lag=10)
print(ljung_box_test)

checkresiduals(model_sarima)

```


Riduzione del rumore: L'aggregazione settimanale potrebbe aver ridotto il rumore nei dati, permettendo al modello di catturare meglio i pattern e le tendenze sottostanti. Questo può portare a previsioni più accurate e a un RMSE inferiore.

Risposta ai dati giornalieri: A volte i dati giornalieri possono contenere fluttuazioni molto rapide e casuali che possono rendere difficile per il modello catturare le vere tendenze. L'aggregazione settimanale potrebbe aver contribuito a mitigare questo effetto.

Stagionalità settimanale: Se i dati mostrano una chiara stagionalità settimanale, l'aggregazione settimanale potrebbe aver permesso al modello di catturare meglio questa stagionalità, portando a previsioni più accurate.

Riduzione della complessità del modello: A volte, i modelli adattati ai dati giornalieri possono diventare troppo complessi a causa delle fluttuazioni quotidiane. L'aggregazione settimanale potrebbe aver semplificato il processo di modellazione, riducendo l'overfitting.

Dimensione del campione: La dimensione del campione potrebbe influenzare l'RMSE. Se hai meno punti dati settimanali rispetto ai dati giornalieri, potresti avere una variabilità minore nell'RMSE settimanale.


PERCHè FUNZIONA?ANALISI SERIE STORICA


Questo funziona perchè se guardiamo attentamente questo grafico sotto , possiamo notare che c'è sia una weekly seasonality come avevamo già visto nella serie storica precedente ma c'è anche una yearly seasonality. inoltre molti punti irregolari che sono dovuti ALLE CHIUSURURE COVIDDI. Dunque il nostri modelli arima possiamo cambiare p e q quanto vogliamo(vedi il markwown precedente) quindi aggiungere tutte le componenti autoregressive ma se i dati presetnano più stagionalità sarà difficile costruire un modello robusto. 
Per questo motivo aggrego per settimana



Adesso plotto prima la serie e già li si vede, per dimiostrarlo nel report si può plottare la serie decomposta dove si vedono chiaramente le 2 stagionalità


COMMENTO FORMALE:

The data from the period between May 4, 2020, and March 3, 2023, exhibited clear signs of multiple seasonality, both on a weekly and annual basis. Traditional ARIMA models, while powerful, are designed to handle single seasonality effectively. Given the complexity and presence of dual seasonality in our daily data, the model struggled to capture the nuances accurately. To address this, data was aggregated on a weekly basis, simplifying the modeling process by focusing on the dominant weekly seasonality and potentially reducing the noise from daily fluctuations

```{r}
plot(time_series, main="Time Series of Lordo Totale", xlab="Year", ylab="Lordo Totale")
```


```{r}


#df <- read.csv("C:/Users/pc/Desktop/r004_data.csv", header = TRUE)#, fileEncoding = "ISO-8859-1")


df$date <- as.Date(df$date, format = "%Y-%m-%d")
start_date <- as.Date("2020-05-04")
end_date <- as.Date("2023-03-3")
filtered_post <- df %>%
  filter(date >= start_date & date <= end_date)

ts_data <- ts(filtered_post$lordo.totale, frequency=365) 
decomposition <- stl(ts_data, s.window="periodic")
plot(decomposition)

```

viz che servono a capire com'è fatta la serie storica
 

```{r}
library(ggplot2)
library(gridExtra)

# Weekly decomposition
ts_data_weekly <- ts(filtered_post$lordo.totale, start=c(2020, 125), frequency=7) # starting from the 125th day of 2020
decomposition_weekly <- stl(ts_data_weekly, s.window="periodic")

# Annual decomposition (for visualization, we'll just reuse the one you've already created)
ts_data_annual <- ts(filtered_post$lordo.totale, frequency=365)
decomposition_annual <- stl(ts_data_annual, s.window="periodic")

# Convert the decompositions to data frames for ggplot
df_weekly <- as.data.frame(decomposition_weekly$time.series)
df_annual <- as.data.frame(decomposition_annual$time.series)

# Plot using ggplot
p1 <- ggplot(df_weekly, aes(x=seq_along(seasonal))) + 
  geom_line(aes(y=seasonal), color="blue") + 
  labs(title="Weekly Seasonality", y="Value") + theme_minimal()

p2 <- ggplot(df_annual, aes(x=seq_along(seasonal))) + 
  geom_line(aes(y=seasonal), color="red") + 
  labs(title="Annual Seasonality", y="Value") + theme_minimal()

# Display plots side by side
grid.arrange(p1, p2, ncol=2)
print(p1)
print(p2)
```

###9.1 Rolling Forecasting Origin ARIMA Weekly

```{r}


data <- ts(filtered_post$lordo.totale, frequency = 1)

result <- rolling_forecast_function(data, model_type = "auto.arima", h = c(7,14,23,70), origins = 20, boxcox = TRUE)
result <- result[[1]]
forecasts <- result[[2]]
model <- result[[3]]

print(paste("Results: ", result))

 
result
```


###9.2 ARIMA SETTIMANALE NON TRASFORMATO

filtered_post <- filtered_post %>%
  mutate(week = lubridate::isoweek(date)) %>%
  group_by(week) %>%
  summarise(lordo.totale = mean(lordo.totale, na.rm = TRUE)




filtered_post_weekly <- filtered_post %>%
  mutate(week = floor_date(date, unit="weeks")) %>%
  group_by(week) %>%
  summarise(total = sum(lordo.totale),  # Adjust this aggregation method if needed
            rosso_zone = ifelse(any(Colore == "Rossa"), 1, 0))

```{r}


# Define training and test data
n <- length(filtered_post$lordo.totale)
train <- filtered_post$lordo.totale[1:(0.8*n)]
test <- filtered_post$lordo.totale[(0.8*n + 1):n]

# Fit a SARIMA(7,1,4)(0,1,0)[7] model
model_sarima <- auto.arima(train, seasonal = TRUE)

# Forecast the test data
forecasts <- forecast(model_sarima, h = length(test))
summary(model_sarima)
# Calculate RMSE for the test set
rmse_test <- sqrt(mean((test - forecasts$mean)^2))
print(paste("RMSE on the test set:", rmse_test))

# Plot residuals
residuals <- residuals(model_sarima)
plot(residuals, main="Residuals from SARIMA model with weekly aggregation")
checkresiduals(model_sarima)
# Perform Ljung-Box test
ljung_box_test <- Box.test(residuals, type = "Ljung-Box", lag=10)
print(ljung_box_test)

checkresiduals(model_sarima)

```


Riduzione del rumore: L'aggregazione settimanale potrebbe aver ridotto il rumore nei dati, permettendo al modello di catturare meglio i pattern e le tendenze sottostanti. Questo può portare a previsioni più accurate e a un RMSE inferiore.

Risposta ai dati giornalieri: A volte i dati giornalieri possono contenere fluttuazioni molto rapide e casuali che possono rendere difficile per il modello catturare le vere tendenze. L'aggregazione settimanale potrebbe aver contribuito a mitigare questo effetto.

Stagionalità settimanale: Se i dati mostrano una chiara stagionalità settimanale, l'aggregazione settimanale potrebbe aver permesso al modello di catturare meglio questa stagionalità, portando a previsioni più accurate.

Riduzione della complessità del modello: A volte, i modelli adattati ai dati giornalieri possono diventare troppo complessi a causa delle fluttuazioni quotidiane. L'aggregazione settimanale potrebbe aver semplificato il processo di modellazione, riducendo l'overfitting.

Dimensione del campione: La dimensione del campione potrebbe influenzare l'RMSE. Se hai meno punti dati settimanali rispetto ai dati giornalieri, potresti avere una variabilità minore nell'RMSE settimanale.









adf_test <- adf.test(filtered_data$lordo.totale, alternative = "stationary")
print(adf_test)
kpss_test <- kpss.test(filtered_data$lordo.totale, null = "Level")
print(kpss_test)






    The ADF test p-value is smaller than the commonly used significance level of 0.05, which suggests that the differenced series is stationary.
    The KPSS test p-value is greater than 0.05, which further confirms that the series is stationary after differencing.












##trasformazione log
ma sembra troppo white noise , non mi convince




```{r}
filtered_post <- filtered_post %>%
  mutate(week = lubridate::isoweek(date)) %>%
  group_by(week) %>%
  summarise(lordo.totale = mean(lordo.totale, na.rm = TRUE))
  
# Log transformation
log_series <- log(filtered_post$lordo.totale)

# 1st order difference transformation
diff_log_series <- diff(log_series)

# Define training and test data
n <- length(diff_log_series)
train <- diff_log_series[1:(0.8*n)]
test <- diff_log_series[(0.8*n + 1):n]

# Fit a SARIMA model on transformed data
model_sarima <- auto.arima(train, seasonal = TRUE)
summary(model_sarima)

# Forecast the test data
forecasts <- forecast(model_sarima, h = length(test))

# Revert transformations for forecasts
last_log_value <- log_series[(0.8*n)]
reverted_diff_forecasts <- diffinv(forecasts$mean, lag=1, differences = 1, xi = last_log_value)
reverted_forecasts <- exp(reverted_diff_forecasts)

# Actual test values (without transformation)
actual_values <- filtered_post$lordo.totale[(0.8*n + 1):n]

# Calculate RMSE for the test set on the original scale
rmse_test <- sqrt(mean((actual_values - reverted_forecasts)^2))
print(paste("RMSE on the test set:", rmse_test))
# Calculate MAPE on the test set
mape_test <- mean(abs((test - forecasts$mean) / test)) * 100
print(paste("MAPE on the test set:", mape_test, "%"))

# Check residuals and perform Ljung-Box test
residuals <- residuals(model_sarima)
plot(residuals, main="Residuals from SARIMA model with weekly aggregation")
checkresiduals(model_sarima)

ljung_box_test <- Box.test(residuals, type = "Ljung-Box", lag=10)
print(ljung_box_test)


```


#boxcox
la funzione ci propone una arima senza nessun termine , mi sembra difficile che la serie sia così perfettamente stazionaria


```{r}
library(forecast)
library(tseries)

# Box-Cox transformation
lambda <- BoxCox.lambda(filtered_post$lordo.totale)
bc_series <- BoxCox(filtered_post$lordo.totale, lambda=lambda)

# 1st order difference transformation
diff_bc_series <- diff(bc_series)

# Define training and test data
n <- length(diff_bc_series)
train <- diff_bc_series[1:(0.8*n)]
test <- diff_bc_series[(0.8*n + 1):n]

# Fit a SARIMA model on transformed data
model_sarima <- auto.arima(train, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
summary(model_sarima)

# Forecast the test data
forecasts <- forecast(model_sarima, h = length(test))

# Revert transformations for forecasts
last_bc_value <- bc_series[(0.8*n)]
reverted_diff_forecasts <- diffinv(forecasts$mean, lag=1, differences = 1, xi = last_bc_value)
reverted_forecasts <- InvBoxCox(reverted_diff_forecasts, lambda)

# Actual test values (without transformation)
actual_values <- filtered_post$lordo.totale[(0.8*n + 1):n]

# Calculate RMSE for the test set on the original scale
rmse_test <- sqrt(mean((actual_values - reverted_forecasts)^2))
print(paste("RMSE on the test set:", rmse_test))

# Calculate MAPE on the test set
mape_test <- mean(abs((actual_values - reverted_forecasts) / actual_values)) * 100
print(paste("MAPE on the test set:", mape_test, "%"))

# Check residuals and perform Ljung-Box test
residuals <- residuals(model_sarima)
plot(residuals, main="Residuals from SARIMA model with Box-Cox transformation")
checkresiduals(model_sarima)

ljung_box_test <- Box.test(residuals, type = "Ljung-Box", lag=10)
print(ljung_box_test)

```









##SARIMA NON STAZIONARIA

QUi la serie storica non è stazionaria eil risultato ci sta'

```{r}
library(forecast)
library(tseries)

# Define training and test data without any transformation
n <- length(filtered_post$lordo.totale)
train <- filtered_post$lordo.totale[1:(0.8*n)]
test <- filtered_post$lordo.totale[(0.8*n + 1):n]

# Fit a SARIMA model
model_sarima <- auto.arima(train, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
summary(model_sarima)

# Forecast the test data
forecasts <- forecast(model_sarima, h = length(test))

# Calculate RMSE for the test set
rmse_test <- sqrt(mean((test - forecasts$mean)^2))
print(paste("RMSE on the test set:", rmse_test))

# Calculate MAPE on the test set
mape_test <- mean(abs((test - forecasts$mean) / test)) * 100
print(paste("MAPE on the test set:", mape_test, "%"))

# Check residuals and perform Ljung-Box test
residuals <- residuals(model_sarima)
plot(residuals, main="Residuals from SARIMA model without transformation")
checkresiduals(model_sarima)

ljung_box_test <- Box.test(residuals, type = "Ljung-Box", lag=10)
print(ljung_box_test)

```
#diff 1

anche qui stesso problema sembra un modello robusto  ma l'arima non  ha termini
autoregressivi e mi sembra molto strano sia così perfetto



```{r}
library(forecast)
library(ggplot2)

# 1. First-differencing the series
diff_series <- diff(filtered_post$lordo.totale, differences = 1)

# 2. Splitting into training and test set (80% training, 20% test)
n <- length(diff_series)
train <- diff_series[1:(0.8*n)]
test <- diff_series[(0.8*n + 1):n]

# 3. Fit an ARIMA model on the differenced data
model_arima <- auto.arima(train, seasonal = TRUE)
summary(model_arima)

# 4. Forecast the test data
forecasts <- forecast(model_arima, h = length(test))

# Revert the differencing to get the forecasts on the original scale
last_original_value <- filtered_post$lordo.totale[(0.8*n)]
reverted_forecasts <- diffinv(forecasts$mean, differences = 1, xi = last_original_value)

# Actual test values
actual_values <- filtered_post$lordo.totale[(0.8*n + 2):n + 1]

# Plot the forecasts against the actual values
#autoplot(forecast(model_arima, h=length(test))) + autolayer(actual_values, series="Actual") + xlab("Time") + ylab("Value")

# 5. Calculate RMSE for the test set on the original scale
rmse_test <- sqrt(mean((actual_values - reverted_forecasts)^2))
print(paste("RMSE on the test set:", rmse_test))

# Calculate MAPE on the test set
mape_test <- mean(abs((actual_values - reverted_forecasts) / actual_values)) * 100
print(paste("MAPE on the test set:", mape_test, "%"))

```






library(ggplot2)
library(forecast)

1. Plotting the Time Series
ts_data <- ts(filtered_post$lordo.totale, frequency = 52)
autoplot(ts_data) +
  ggtitle("Time Series Plot of filtered_post$lordo.totale") +
  xlab("Time") +
  ylab("Value")

 2. Plotting the ACF
acf(ts_data, main="Autocorrelation Function")

3. Plotting the PACF
pacf(ts_data, main="Partial Autocorrelation Function")




##FOURIER WEEKLY
```{r}
# Generate Fourier terms for the entire dataset
fourier_terms <- fourier(ts(filtered_post$lordo.totale, frequency=52), K=3)

# Splitting the data and Fourier terms
train_size <- floor(0.8 * nrow(filtered_post))
train <- filtered_post[1:train_size, ]
test <- filtered_post[(train_size + 1):nrow(filtered_post), ]

train_fourier <- fourier_terms[1:train_size, ]
test_fourier <- fourier_terms[(train_size + 1):nrow(fourier_terms), ]

# Convert the training data to a time series object
train_ts <- ts(train$lordo.totale, frequency=52)
set.seed(2)
# Fit ARIMA model with Fourier terms
model_fourirer <- auto.arima(train_ts, xreg=train_fourier, lambda="auto", stepwise=TRUE, seasonal = TRUE, approximation=FALSE)

# In-sample forecast to get predictions on the training set
in_sample_forecasts <- fitted(model_fourirer)
rmse_train <- sqrt(mean((train$lordo.totale - in_sample_forecasts)^2))
print(paste("RMSE on the training set:", rmse_train))
summary(model_fourirer)
# Forecast on the test set
forecasts <- forecast(model_fourirer, xreg=test_fourier, h=nrow(test))
rmse_test <- sqrt(mean((test$lordo.totale - forecasts$mean)^2))
print(paste("RMSE on the test set:", rmse_test))
accuracy(forecasts$mean, test$lordo.totale)
checkresiduals(model_fourirer)
plot(resid(model_fourirer))
```

library(lubridate)
library(dplyr)

 Assuming your date column in filtered_post is named 'date'
filtered_post_weekly <- filtered_post %>%
  mutate(week = floor_date(date, unit="weeks")) %>%
  group_by(week) %>%
  summarise(total = sum(lordo.totale),  # Adjust this aggregation method if needed
            rosso_zone = ifelse(any(Colore == "Rossa"), 1, 0))

 View the transformed dataframe
head(filtered_post_weekly)

##WEEKLY ARIMAX+ZONA ROSSA

```{r}
library(dplyr)
library(lubridate)
library(forecast)

 #Filter, add the dummy variable for 'Colore', and aggregate by ISO week
filtered_post <- df %>%
  filter(date >= start_date & date <= end_date) %>%
  mutate(rosso_dummy = ifelse(Colore == "Rossa", 1, 0)) %>%
  mutate(week = lubridate::isoweek(date)) %>%
  group_by(week) %>%
  summarise(lordo.totale = mean(lordo.totale, na.rm = TRUE),
            rosso_dummy = sum(rosso_dummy)) %>%
  mutate(rosso_dummy = ifelse(rosso_dummy > 0, 1, 0))

n <- nrow(filtered_post)

 #Partition data into training and test sets
train_size <- round(0.8 * n)
train <- filtered_post[1:train_size, ]
test <- filtered_post[(train_size + 1):n, ]

#Create a time series object for the training set and the corresponding dummy
train_ts <- ts(train$lordo.totale, frequency = 52)
dummy_train <- train$rosso_dummy
#Fit the ARIMA model with the 'Rossa' dummy variable
model_sarima <- auto.arima(train_ts, seasonal = TRUE, xreg = dummy_train)

#Forecast on the test set using the model
forecasts <- forecast(model_sarima, h = nrow(test), xreg = test$rosso_dummy)

#Output model summary and calculate RMSE
summary(model_sarima)
rmse_test <- sqrt(mean((test$lordo.totale - forecasts$mean)^2))
print(paste("RMSE on the test set:", rmse_test))

 #Plot residuals
residuals <- residuals(model_sarima)
plot(residuals, main = "Residuals from SARIMA model with Rossa Dummy")
checkresiduals(model_sarima)
ljung_box_test <- Box.test(residuals, type = "Ljung-Box", lag = 10)
print(ljung_box_test)


```



<!-- ````markdown -->
<!-- library(dplyr) -->
<!-- library(lubridate) -->
<!-- library(forecast) -->

<!--  Filter, add the dummy variable for 'Colore', and aggregate by ISO week -->
<!-- filtered_post <- df %>% -->
<!--   filter(date >= start_date & date <= end_date) %>% -->
<!--   mutate(rosso_dummy = ifelse(Colore == "Rossa", 1, 0)) %>% -->
<!--   mutate(week = lubridate::isoweek(date)) %>% -->
<!--   group_by(week) %>% -->
<!--   summarise(lordo.totale = mean(lordo.totale, na.rm = TRUE), -->
<!--             rosso_dummy = sum(rosso_dummy)) %>% -->
<!--   mutate(rosso_dummy = ifelse(rosso_dummy > 0, 1, 0)) -->

<!-- n <- nrow(filtered_post) -->

<!--  Partition data into training and test sets -->
<!-- train_size <- round(0.8 * n) -->
<!-- train <- filtered_post[1:train_size, ] -->
<!-- test <- filtered_post[(train_size + 1):n, ] -->

<!-- Create a time series object for the training set and the corresponding dummy -->
<!-- train_ts <- ts(train$lordo.totale, frequency = 52) -->
<!-- dummy_train <- train$rosso_dummy -->
<!--  Fit the ARIMA model with the 'Rossa' dummy variable -->
<!-- model_sarima <- auto.arima(train_ts, seasonal = TRUE, xreg = dummy_train) -->

<!--  Forecast on the test set using the model -->
<!-- forecasts <- forecast(model_sarima, h = nrow(test), xreg = test$rosso_dummy) -->

<!-- Output model summary and calculate RMSE -->
<!-- summary(model_sarima) -->
<!-- rmse_test <- sqrt(mean((test$lordo.totale - forecasts$mean)^2)) -->
<!-- print(paste("RMSE on the test set:", rmse_test)) -->

<!--  Plot residuals -->
<!-- residuals <- residuals(model_sarima) -->
<!-- plot(residuals, main = "Residuals from SARIMA model with Rossa Dummy") -->
<!-- checkresiduals(model_sarima) -->
<!-- ljung_box_test <- Box.test(residuals, type = "Ljung-Box", lag = 10) -->
<!-- print(ljung_box_test) -->


<!-- ``` -->
Series: train_ts 
Regression with ARIMA(1,0,0) errors 

Coefficients:
         ar1   intercept        xreg
      0.7726  33663.6273  -1686.6749
s.e.  0.0934    878.7623    677.8628

sigma^2 = 2047049:  log likelihood = -363.66
AIC=735.33   AICc=736.41   BIC=742.28

Training set error measures:
                    ME     RMSE      MAE        MPE     MAPE MASE      ACF1
Training set -18.13278 1378.706 1016.875 -0.2456628 3.135501  NaN 0.1855808
[1] "RMSE on the test set: 4680.52546746011"

	Ljung-Box test

data:  Residuals from Regression with ARIMA(1,0,0) errors
Q* = 4.1358, df = 7, p-value = 0.764

Model df: 1.   Total lags used: 8


	Box-Ljung test

data:  residuals
X-squared = 7.6463, df = 10, p-value = 0.6633






#10 TBAts
```{r}
tbats_model <- tbats(train_vendite)
forecast_tbats <- forecast(tbats_model, h=length(test_vendite))
rmse_tbats <- sqrt(mean((test_vendite - forecast_tbats$mean)^2))
mape_tbats <- mean(abs((test_vendite - forecast_tbats$mean) / test_vendite)) * 100
print(paste("RMSE for TBATS:", rmse_tbats))
print(paste("MAPE for TBATS:", mape_tbats, "%"))
print(tbats_model)
```


#RESULT


## RANDOM FOREST RESULT

```{r}
filtered_df_res$lag_1 <- lag(filtered_df_res$lordo.totale, 1)
filtered_df_res$lag_7 <- lag(filtered_df_res$lordo.totale, 7)

filtered_df$lag_1 <- lag(filtered_df$lordo.totale, 1)
filtered_df$lag_7 <- lag(filtered_df$lordo.totale, 7)
filtered_df
filtered_df_res <- filtered_df_res[-(1:7), ]
filtered_df_res <- na.omit(filtered_df_res)

filtered_df <- filtered_df[-(1:7), ]
filtered_df <- na.omit(filtered_df)


```


#PROPHET RESULTS
```{r}

library("MLmetrics")
library(Metrics) 
r = c('R000', 'R001', 'R002', 'R003','R004', 'R005')

for (re in r) {
  d <- filtered_post %>%
    filter(ristorante == re & lordo.totale != 0)

    train_size <- floor(0.8 * nrow(d))
    train <- d[1:train_size, ]
    test <- d[(train_size + 1):nrow(d), ]
    # Convert the training data to a time series object with a frequency of 7 (weekly seasonality)
    train_ts <- ts(train$lordo.totale, frequency=7)
    prophet_df <- data.frame(ds = train$date, y = train$lordo.totale)
    set.seed(13)
    model_prophet <- prophet(prophet_df)
    future <- make_future_dataframe(model_prophet, periods = nrow(test))
    forecasts_prophet <- predict(model_prophet, future)
    
    
    accuracy(forecasts_prophet$yhat[1:nrow(train)], train$lordo.totale)
    accuracy(forecasts_prophet$yhat[(nrow(train)+1):nrow(forecasts_prophet)], test$lordo.totale)
    train_rmse <- sqrt(mean((train$lordo.totale - forecasts_prophet$yhat[1:nrow(train)])^2))
    test_rmse <- sqrt(mean((test$lordo.totale - forecasts_prophet$yhat[(nrow(train)+1):nrow(forecasts_prophet)])^2))
    test_mape<- mean(abs((test$lordo.totale - forecasts_prophet$yhat[(nrow(train)+1):nrow(forecasts_prophet)]) / test$lordo.totale), na.rm = TRUE) * 100
    print(paste("RMSE on the test set Func:", test_rmse))
    print(paste("MAPE on the test set Func:", test_mape))
    print(paste("RMSE on the test set:", rmse(test$lordo.totale, forecasts_prophet$yhat[(nrow(train)+1):nrow(forecasts_prophet)])))
    print(paste("MAPE on the test set:", MAPE(test$lordo.totale, forecasts_prophet$yhat[(nrow(train)+1):nrow(forecasts_prophet)])))
    accuracy(forecasts_prophet$yhat[(nrow(train)+1):nrow(forecasts_prophet)], test$lordo.totale)


  print(paste("Mean in predicted period:", mean(forecasts_prophet$yhat[1:nrow(train)])))
  print(paste("Total in predicted period:", mean(forecasts_prophet$yhat[1:nrow(train)]) *70))

}
```

#ETS RESULTS
```{r}

library("MLmetrics")
library(Metrics) 
r <- c("R000","R001", "R002", "R003", "R004", "R005")

for (re in r) {
  d <- filtered_post %>%
    filter(ristorante == re & lordo.totale != 0)

    train_size <- floor(0.8 * nrow(d))
    train <- d[1:train_size, ]
    test <- d[(train_size + 1):nrow(d), ]
    # Convert the training data to a time series object with a frequency of 7 (weekly seasonality)
    train_ts <- ts(train$lordo.totale, frequency=7)
model_ets <- ets(train_ts, model="ZZZ")#, damped=NULL, lambda=TRUE,allow.multiplicative.trend=TRUE,biasadj=TRUE)
print("ETS Model Summary:")
summary(model_ets)
forecasts_ets <- forecast(model_ets, h=nrow(test))
plot(forecasts_ets)
print("Test Set Accuracy Metrics for ETS:")
accuracy(forecasts_ets$mean, test$lordo.totale)

# Calculate RMSE for the test set using ETS
rmse_value_ets <- sqrt(mean((test$lordo.totale - forecasts_ets$mean)^2))
print(paste("RMSE on the test set using ETS:", rmse_value_ets))
mape_test_ets <- mean(abs((test$lordo.totale - forecasts_ets$mean) / test$lordo.totale)) * 100

print(paste("MAPE on the test set using ETS:", mape_test_ets))


  print(paste("Mean in predicted period:", mean(forecasts_ets$mean)))
  print(paste("Total in predicted period:", mean(forecasts_ets$mean)*70))

}
```



#TBATS RESULTS

```{r}

library("MLmetrics")
library(Metrics) 

for (re in r) {
  d <- filtered_post %>%
    filter(ristorante == re & lordo.totale != 0)

    train_size <- floor(0.8 * nrow(d))
    train <- d[1:train_size, ]
    test <- d[(train_size + 1):nrow(d), ]
    # Convert the training data to a time series object with a frequency of 7 (weekly seasonality)
train_ts <- ts(train$lordo.totale, frequency=7)
set.seed(1)
model_tbats <- tbats(train_ts)
print("TBATS Model Summary:")
summary(model_tbats)
forecasts_tbats <- forecast(model_tbats, h=nrow(test))
# Calculate RMSE for the test set using ETS
rmse_value_ets <- sqrt(mean((test$lordo.totale - forecasts_tbats$mean)^2))
print(paste("RMSE on the test set using ETS:", rmse_value_ets))
mape_test_ets <- mean(abs((test$lordo.totale - forecasts_tbats$mean) / test$lordo.totale)) * 100

print(paste("MAPE on the test set using ETS:", mape_test_ets))


print(paste("Mean in predicted period:", mean(forecasts_tbats$mean)))
print(paste("Total in predicted period:", mean(forecasts_tbats$mean)*70))

}
```



#BAYESIAN MODELS RESULTS


```{r}
r = c('R000', 'R001', 'R002', 'R003','R004', 'R005')

for (re in r) {

filtered_post <- filtered_post %>%
  mutate(
    lordo.totale_lag1 = lag(lordo.totale, 1),
    lordo.totale_lag2 = lag(lordo.totale, 2),
    lordo.totale_lag7 = lag(lordo.totale, 7)
  )
  d <- filtered_post %>%
    filter(ristorante == re & lordo.totale != 0)

# Encode Giorno as weekend or not, assuming 6 and 7 represent weekend days.
# filtered_df$Weekend <- ifelse(filtered_df$Giorno %in% c(6, 7), 1, 0)



# Split the data
    train_size <- floor(0.8 * nrow(d))
    train <- d[1:train_size, ]
    test <- d[(train_size + 1):nrow(d), ]
set.seed(1)

# Fit a Bayesian linear regression model with Giorno, multiple lags and possibly weekend
model_bayesian_with_lags <- stan_glm(lordo.totale ~ Giorno +Festivo_New+Festivo_or_Weekend_New+ Weekend_New+Season+ lordo.totale_lag1 + lordo.totale_lag2 + lordo.totale_lag7 + Colore + Lockdown , 
                                     data = train)


# Predict on the test set
predictions <- predict(model_bayesian_with_lags, newdata = test)

# Compute RMSE
RMSE <- sqrt(mean((predictions - test$lordo.totale)^2))

# Calculate RMSE for the test set using ETS
rmse_value_ets <- sqrt(mean((predictions - test$lordo.totale)^2))
print(paste("RMSE on the test set using ETS:", rmse_value_ets))
mape_test_ets <- mean(abs((test$lordo.totale - predictions) / test$lordo.totale)) * 100

print(paste("MAPE on the test set using ETS:", mape_test_ets))


  print(paste("Mean in predicted period:", mean(predictions)))
  print(paste("Total in predicted period:", mean(predictions)*70))

}

predictions
```




```{r}
g = forecasts_prophet$yhat[nrow(train) : length(forecasts_prophet$yhat)]
g[1]
g = g[2:length(g)]
g  
MAPE(g, test$lordo.totale)

rmse(test$lordo.totale, g)

 
mean(abs((test$lordo.totale - forecasts_prophet$yhat[(nrow(train)+1):nrow(forecasts_prophet)]) / test$lordo.totale), na.rm = TRUE) * 100

```

#HOLT WINTERS RESULTS
```{r}
library("MLmetrics")
library(Metrics) 

for (re in r) {
  d <- filtered_post %>%
    filter(ristorante == re & lordo.totale != 0)

    train_size <- floor(0.8 * nrow(d))
    train <- d[1:train_size, ]
    test <- d[(train_size + 1):nrow(d), ]
    # Convert the training data to a time series object with a frequency of 7 (weekly seasonality)
train_ts <- ts(train$lordo.totale, frequency=7)
set.seed(1)








vendite_ts <- ts(d$lordo.totale,  frequency = (365.25/7))
train_size <- floor(0.8 * length(vendite_ts))
train_vendite <- vendite_ts[1:train_size]
test_vendite <- vendite_ts[(train_size + 1):length(vendite_ts)]
hw_model <- HoltWinters(train_vendite,seasonal = "multiplicative" ,beta = FALSE, gamma = FALSE)
forecast_hw <- forecast(hw_model, h=length(test_vendite))






model_tbats <- tbats(train_ts)
print("TBATS Model Summary:")
summary(model_tbats)
# Calculate RMSE for the test set using ETS
rmse_value_ets <- sqrt(mean((test$lordo.totale - forecast_hw$mean)^2))
print(paste("RMSE on the test set using ETS:", rmse_value_ets))
mape_test_ets <- mean(abs((test$lordo.totale - forecast_hw$mean) / test$lordo.totale)) * 100

print(paste("MAPE on the test set using ETS:", mape_test_ets))



}
```
#RANDOM FOREST RESULT
```{r}
library(randomForest)
library(dplyr)
filtered_post <- filtered_post %>%
  mutate(
    lordo.totale_lag1 = lag(lordo.totale, 1),
    lordo.totale_lag2 = lag(lordo.totale, 2),
    lordo.totale_lag7 = lag(lordo.totale, 7)
  )
filtered_post <- filtered_post[-(1:7), ]
filtered_post <- na.omit(filtered_post)
# Adjust the size of the plotting device
options(repr.plot.width = 8, repr.plot.height = 6)

r <- c("R000", "R001", "R002", "R003", "R004", "R005")

# Create a layout with 2 columns and 3 rows
par(mfrow = c(3, 2))

# Set smaller margins
par(mar = c(2.5, 2.5, 2.5, 2.5))

for (re in r) {
  d <- filtered_post %>%
    filter(ristorante == re & lordo.totale != 0)

  train_size <- floor(0.8 * nrow(d))
  train_set <- d[1:train_size, ]
  test_set <- d[(train_size + 1):nrow(d), ]
  print(train_set)
  train_set
  model <- randomForest(lordo.totale ~ Giorno + Weekend_New + Festivo_or_Weekend_New + lordo.totale_lag7 + lordo.totale_lag2 + lordo.totale_lag1 + Colore + Lockdown, data = train_set)
  #print(paste("Feature Importance ", re, ": "))
  # Assuming your variable importance plot data frame is named 'var_imp_df'
  #rownames(model$importance) <- c("Day", "Weekend", "Holiday or Weekend", "Lag 7", "Lag 2", "Lag 1", "Color", "Lockdown")
  #print(model$importance)

  # Now create the variable importance plot
  #varImpPlot(model, main = paste(re), pch = 19, col = "orange", cex = 1.5)
predictions <- predict(model, newdata=test_set)
importance(rf_model)
varImpPlot(rf_model)
rmse <- sqrt(mean((test_set$lordo.totale - predictions)^2))

print(paste("RMSE on the test set :", rmse))
mape_test_ets <- mean(abs((test_set$lordo.totale - predictions) / test_set$lordo.totale)) * 100

print(paste("MAPE on the test set:", mape_test_ets))

}

# Reset the layout and margins to the default
par(mfrow = c(1, 1))
par(mar = c(5, 4, 4, 2) + 0.1)

```

#ARIMAX RESULT
```{r}


for (re in r) {
  d <- filtered_post %>%
    filter(ristorante == re & lordo.totale != 0)

d$lag_1 <- lag(d$lordo.totale, 1)
d$lag_2 <- lag(d$lordo.totale, 2)
d$lag_7 <- lag(d$lordo.totale, 7)
d <- d[8:nrow(d), ]
colore_dummies <- model.matrix(~ Colore, data = d)[,-1]  # Drop first column
giorno_dummies <- model.matrix(~ Giorno, data = d)[,-1] 
combined_xreg <- cbind(colore_dummies, giorno_dummies, d$lag_1, d$lag_2, d$lag_7)
vendite_ts <- ts(d$lordo.totale, frequency = 1)
train_size <- floor(0.8 * length(vendite_ts))
train_vendite <- vendite_ts[1:train_size]
test_vendite <- vendite_ts[(train_size + 1):length(vendite_ts)]
train_predictors <- combined_xreg[1:train_size, ]
test_predictors <- combined_xreg[(train_size + 1):nrow(combined_xreg), ]
fit <- auto.arima(train_vendite, xreg=train_predictors, seasonal=TRUE)
forecasted <- forecast(fit, xreg=test_predictors, h=length(test_vendite))
rmse <- sqrt(mean((test_vendite - forecasted$mean)^2))
print(paste("RMSE for SARIMAX with 'Colore', 'Giorno', and Lagged Variables:", rmse))
mape_test_ets <- mean(abs((test_vendite - forecasted$mean) / test_vendite)) * 100

print(paste("MAPE on the test set:", mape_test_ets))

}
```
```{r}


for (re in r) {
  d <- filtered_post %>%
    filter(ristorante == re & lordo.totale != 0)

  
  
vendite_ts <- ts(d$lordo.totale, frequency = 365.25/7)
K <- 3
fourier_terms <- fourier(vendite_ts, K=K)
colore_dummies <- model.matrix(~ Colore - 1, data = d)[,-1]  # drop the first column
giorno_dummies <- model.matrix(~ Giorno - 1, data = d)[,-1]  # drop the first column
combined_xreg <- cbind(fourier_terms, colore_dummies, giorno_dummies)
train_size <- floor(0.94 * length(vendite_ts))
train_vendite <- vendite_ts[1:train_size]
test_vendite <- vendite_ts[(train_size + 1):length(vendite_ts)]
train_regressors <- combined_xreg[1:train_size,]
test_regressors <- combined_xreg[(train_size + 1):nrow(combined_xreg),]
fit <- auto.arima(train_vendite, xreg=train_regressors, seasonal=TRUE)
forecasted <- forecast(fit, xreg=test_regressors, h=length(test_vendite))
rmse <- sqrt(mean((test_vendite - forecasted$mean)^2))
print(paste("RMSE for ARIMA with Fourier terms, 'Colore', and 'Giorno' dummies:", rmse))
mape_test_ets <- mean(abs((test_vendite - forecasted$mean) / test_vendite)) * 100

print(paste("MAPE on the test set:", mape_test_ets))
}
```

